{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finetune segmentation(with test)-last",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guanlin-chen08/laravel-test/blob/main/finetune_segmentation(with_test)_last.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFfvAsD7HZyD"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5-pO-FBHa2H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d92cef4-913d-4be0-ad3f-252bf5fa6a45"
      },
      "source": [
        "!pip install -U segmentation-models-pytorch albumentations --user"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting segmentation-models-pytorch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/54/8953f9f7ee9d451b0f3be8d635aa3a654579abf898d17502a090efe1155a/segmentation_models_pytorch-0.1.3-py3-none-any.whl (66kB)\n",
            "\r\u001b[K     |█████                           | 10kB 25.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 20kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 30kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 40kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 51kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 61kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 5.1MB/s \n",
            "\u001b[?25hCollecting albumentations\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/58/63fb1d742dc42d9ba2800ea741de1f2bc6bb05548d8724aa84794042eaf2/albumentations-0.5.2-py3-none-any.whl (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 6.7MB/s \n",
            "\u001b[?25hCollecting timm==0.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/2d/39ecc56fbb202e1891c317e8e44667299bc3b0762ea2ed6aaaa2c2f6613c/timm-0.3.2-py3-none-any.whl (244kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 12.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: torchvision>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from segmentation-models-pytorch) (0.8.1+cu101)\n",
            "Collecting pretrainedmodels==0.7.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/0e/be6a0e58447ac16c938799d49bfb5fb7a80ac35e137547fc6cee2c08c4cf/pretrainedmodels-0.7.4.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.7MB/s \n",
            "\u001b[?25hCollecting efficientnet-pytorch==0.6.3\n",
            "  Downloading https://files.pythonhosted.org/packages/b8/cb/0309a6e3d404862ae4bc017f89645cf150ac94c14c88ef81d215c8e52925/efficientnet_pytorch-0.6.3.tar.gz\n",
            "Collecting imgaug>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/b1/af3142c4a85cba6da9f4ebb5ff4e21e2616309552caca5e8acefe9840622/imgaug-0.4.0-py2.py3-none-any.whl (948kB)\n",
            "\u001b[K     |████████████████████████████████| 952kB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from albumentations) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: scikit-image>=0.16.1 in /usr/local/lib/python3.6/dist-packages (from albumentations) (0.16.2)\n",
            "Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.6/dist-packages (from albumentations) (3.13)\n",
            "Collecting opencv-python-headless>=4.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/e9/57d869561389884136be65a2d1bc038fe50171e2ba348fda269a4aab8032/opencv_python_headless-4.4.0.46-cp36-cp36m-manylinux2014_x86_64.whl (36.7MB)\n",
            "\u001b[K     |████████████████████████████████| 36.7MB 88kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from albumentations) (1.19.4)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.0 in /usr/local/lib/python3.6/dist-packages (from timm==0.3.2->segmentation-models-pytorch) (1.7.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3.0->segmentation-models-pytorch) (7.0.0)\n",
            "Collecting munch\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: imageio in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: opencv-python in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (4.1.2.30)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: Shapely in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (1.7.1)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.16.1->albumentations) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.16.1->albumentations) (2.5)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.0->timm==0.3.2->segmentation-models-pytorch) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.0->timm==0.3.2->segmentation-models-pytorch) (0.8)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0->timm==0.3.2->segmentation-models-pytorch) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations) (4.4.2)\n",
            "Building wheels for collected packages: pretrainedmodels, efficientnet-pytorch\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-cp36-none-any.whl size=60963 sha256=26f292cb72b88c3ee745e5367380fff29a95b4641eb8df2abba40ead49c4b019\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/df/63/62583c096289713f22db605aa2334de5b591d59861a02c2ecd\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-cp36-none-any.whl size=12421 sha256=4f674c3b2cbfb96e67e2a8b0ec30c4c3b80da05c2f62acf307c3c79feea10fdb\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/1e/a9/2a578ba9ad04e776e80bf0f70d8a7f4c29ec0718b92d8f6ccd\n",
            "Successfully built pretrainedmodels efficientnet-pytorch\n",
            "Installing collected packages: timm, munch, pretrainedmodels, efficientnet-pytorch, segmentation-models-pytorch, imgaug, opencv-python-headless, albumentations\n",
            "Successfully installed albumentations-0.5.2 efficientnet-pytorch-0.6.3 imgaug-0.4.0 munch-2.5.0 opencv-python-headless-4.4.0.46 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.1.3 timm-0.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4QZTcy5cEB3"
      },
      "source": [
        "import os\r\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import cv2\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jd0pFnRud5Ky"
      },
      "source": [
        "# **Dataloader**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xErfk0fCcmi1"
      },
      "source": [
        "from torch.utils.data import DataLoader\r\n",
        "from torch.utils.data import Dataset as BaseDataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tea3FscCdx10"
      },
      "source": [
        "#**Augmentations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d4nD8gGdzOj"
      },
      "source": [
        "import albumentations as albu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pvu3m4chegiI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "outputId": "d7cd7ce4-89c3-4002-9a0d-7c7d66be9620"
      },
      "source": [
        "import torch\r\n",
        "import numpy as np\r\n",
        "import segmentation_models_pytorch as smp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-79bcc1e7569a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msegmentation_models_pytorch\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'segmentation_models_pytorch'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voS5W_lUyGBG"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from torch.utils.tensorboard import SummaryWriter\r\n",
        "import numpy as np\r\n",
        "logdir = './logs'\r\n",
        "writer = SummaryWriter('./logs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOw73fOpaLcz"
      },
      "source": [
        "import os\r\n",
        "import torch\r\n",
        "import segmentation_models_pytorch as smp\r\n",
        "import numpy as np\r\n",
        "import cv2\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import albumentations as albu\r\n",
        "import torch.nn as nn\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torch.utils.data import Dataset as BaseDataset\r\n",
        "\r\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\r\n",
        "DATA_DIR = './gdrive/MyDrive/Colab Notebooks/SYNTHIA4/'\r\n",
        "\r\n",
        "\r\n",
        "x_train_dir = os.path.join(DATA_DIR, 'Train/RGB')\r\n",
        "y_train_dir = os.path.join(DATA_DIR, 'Train/SemSeg')\r\n",
        "\r\n",
        "x_valid_dir = os.path.join(DATA_DIR, 'Validation/RGB')\r\n",
        "y_valid_dir = os.path.join(DATA_DIR, 'Validation/SemSeg')\r\n",
        "\r\n",
        "\r\n",
        "# helper function for data visualization\r\n",
        "def visualize(**images):\r\n",
        "    \"\"\"PLot images in one row.\"\"\"\r\n",
        "    n = len(images)\r\n",
        "    plt.figure(figsize=(16, 5))\r\n",
        "    for i, (name, image) in enumerate(images.items()):\r\n",
        "        plt.subplot(1, n, i + 1)\r\n",
        "        plt.xticks([])\r\n",
        "        plt.yticks([])\r\n",
        "        plt.title(' '.join(name.split('_')).title())\r\n",
        "        plt.imshow(image)\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "def get_training_augmentation():\r\n",
        "    train_transform = [\r\n",
        "\r\n",
        "        albu.HorizontalFlip(p=0.5),\r\n",
        "\r\n",
        "        albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\r\n",
        "\r\n",
        "        albu.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),\r\n",
        "        albu.RandomCrop(height=320, width=320, always_apply=True),\r\n",
        "\r\n",
        "        albu.IAAAdditiveGaussianNoise(p=0.2),\r\n",
        "        albu.IAAPerspective(p=0.5),\r\n",
        "\r\n",
        "        albu.OneOf(\r\n",
        "            [\r\n",
        "                albu.CLAHE(p=1),\r\n",
        "                albu.RandomBrightness(p=1),\r\n",
        "                albu.RandomGamma(p=1),\r\n",
        "            ],\r\n",
        "            p=0.9,\r\n",
        "        ),\r\n",
        "\r\n",
        "        albu.OneOf(\r\n",
        "            [\r\n",
        "                albu.IAASharpen(p=1),\r\n",
        "                albu.Blur(blur_limit=3, p=1),\r\n",
        "                albu.MotionBlur(blur_limit=3, p=1),\r\n",
        "            ],\r\n",
        "            p=0.9,\r\n",
        "        ),\r\n",
        "\r\n",
        "        albu.OneOf(\r\n",
        "            [\r\n",
        "                albu.RandomContrast(p=1),\r\n",
        "                albu.HueSaturationValue(p=1),\r\n",
        "            ],\r\n",
        "            p=0.9,\r\n",
        "        ),\r\n",
        "    ]\r\n",
        "    return albu.Compose(train_transform)\r\n",
        "\r\n",
        "\r\n",
        "def get_validation_augmentation():\r\n",
        "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\r\n",
        "    test_transform = [\r\n",
        "        albu.PadIfNeeded(480, 640)\r\n",
        "    ]\r\n",
        "    return albu.Compose(test_transform)\r\n",
        "\r\n",
        "\r\n",
        "def to_tensor(x, **kwargs):\r\n",
        "    return x.transpose(2, 0, 1).astype('float32')\r\n",
        "\r\n",
        "\r\n",
        "def get_preprocessing(preprocessing_fn):\r\n",
        "    \"\"\"Construct preprocessing transform\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        preprocessing_fn (callbale): data normalization function \r\n",
        "            (can be specific for each pretrained neural network)\r\n",
        "    Return:\r\n",
        "        transform: albumentations.Compose\r\n",
        "    \r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    _transform = [\r\n",
        "        albu.Lambda(image=preprocessing_fn),\r\n",
        "        albu.Lambda(image=to_tensor, mask=to_tensor),\r\n",
        "    ]\r\n",
        "    return albu.Compose(_transform)\r\n",
        "\r\n",
        "class Dataset(BaseDataset):\r\n",
        "    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        images_dir (str): path to images folder\r\n",
        "        masks_dir (str): path to segmentation masks folder\r\n",
        "        class_values (list): values of classes to extract from segmentation mask\r\n",
        "        augmentation (albumentations.Compose): data transfromation pipeline \r\n",
        "            (e.g. flip, scale, etc.)\r\n",
        "        preprocessing (albumentations.Compose): data preprocessing \r\n",
        "            (e.g. noralization, shape manipulation, etc.)\r\n",
        "    \r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    CLASSES = [ 'misc', 'sky', 'building', 'road', 'sidewalk', \r\n",
        "                'fence', 'vegetation', 'pole', 'car', 'sign', \r\n",
        "                'pedestrian', 'cyclist', 'lane-marking']\r\n",
        "\r\n",
        "    \r\n",
        "    def __init__(\r\n",
        "            self, \r\n",
        "            images_dir, \r\n",
        "            masks_dir, \r\n",
        "            classes=None, \r\n",
        "            augmentation=None, \r\n",
        "            preprocessing=None,\r\n",
        "    ):\r\n",
        "        self.ids = os.listdir(images_dir)\r\n",
        "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\r\n",
        "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\r\n",
        "        \r\n",
        "        # convert str names to class values on masks\r\n",
        "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\r\n",
        "        \r\n",
        "        self.augmentation = augmentation\r\n",
        "        self.preprocessing = preprocessing\r\n",
        "    \r\n",
        "    def __getitem__(self, i):\r\n",
        "        \r\n",
        "        # read data\r\n",
        "        image = cv2.imread(self.images_fps[i])\r\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\r\n",
        "        mask = cv2.imread(self.masks_fps[i], 0)\r\n",
        "        \r\n",
        "        # extract certain classes from mask (e.g. cars)\r\n",
        "        masks = [(mask == v) for v in self.class_values]\r\n",
        "        mask = np.stack(masks, axis=-1).astype('float')\r\n",
        "        \r\n",
        "        # apply augmentations\r\n",
        "        if self.augmentation:\r\n",
        "            sample = self.augmentation(image=image, mask=mask)\r\n",
        "            image, mask = sample['image'], sample['mask']\r\n",
        "        \r\n",
        "        # apply preprocessing\r\n",
        "        if self.preprocessing:\r\n",
        "            sample = self.preprocessing(image=image, mask=mask)\r\n",
        "            image, mask = sample['image'], sample['mask']\r\n",
        "            \r\n",
        "        return image, mask\r\n",
        "        \r\n",
        "    def __len__(self):\r\n",
        "        return len(self.ids)\r\n",
        "\r\n",
        "\r\n",
        "ENCODER = 'se_resnext50_32x4d'\r\n",
        "ENCODER_WEIGHTS = 'imagenet'\r\n",
        "CLASSES = [ 'misc', 'sky', 'building', 'road', 'sidewalk', \r\n",
        "            'fence', 'vegetation', 'pole', 'car', 'sign', \r\n",
        "            'pedestrian', 'cyclist', 'lane-marking']\r\n",
        "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multicalss segmentation\r\n",
        "DEVICE = 'cuda'\r\n",
        "\r\n",
        "# create segmentation model with pretrained encoder\r\n",
        "model = smp.FPN(\r\n",
        "    encoder_name=ENCODER, \r\n",
        "    encoder_weights=ENCODER_WEIGHTS, \r\n",
        "    classes=len(CLASSES), \r\n",
        "    activation=ACTIVATION,\r\n",
        ")\r\n",
        "\r\n",
        "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\r\n",
        "\r\n",
        "\r\n",
        "train_dataset = Dataset(\r\n",
        "    x_train_dir, \r\n",
        "    y_train_dir, \r\n",
        "    augmentation=get_training_augmentation(), \r\n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\r\n",
        "    classes=CLASSES,\r\n",
        ")\r\n",
        "\r\n",
        "valid_dataset = Dataset(\r\n",
        "    x_valid_dir, \r\n",
        "    y_valid_dir, \r\n",
        "    augmentation=get_validation_augmentation(), \r\n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\r\n",
        "    classes=CLASSES,\r\n",
        ")\r\n",
        "\r\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=12)\r\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False, num_workers=4)\r\n",
        "\r\n",
        "loss = smp.utils.losses.DiceLoss()\r\n",
        "metrics = [\r\n",
        "    smp.utils.metrics.IoU(threshold=0.5),\r\n",
        "]\r\n",
        "\r\n",
        "optimizer = torch.optim.Adam([ \r\n",
        "    dict(params=model.parameters(), lr=0.0001),\r\n",
        "])\r\n",
        "\r\n",
        "# create epoch runners \r\n",
        "# it is a simple loop of iterating over dataloader`s samples\r\n",
        "train_epoch = smp.utils.train.TrainEpoch(\r\n",
        "    model, \r\n",
        "    loss=loss, \r\n",
        "    metrics=metrics, \r\n",
        "    optimizer=optimizer,\r\n",
        "    device=DEVICE,\r\n",
        "    verbose=True,\r\n",
        ")\r\n",
        "\r\n",
        "valid_epoch = smp.utils.train.ValidEpoch(\r\n",
        "    model, \r\n",
        "    loss=loss, \r\n",
        "    metrics=metrics, \r\n",
        "    device=DEVICE,\r\n",
        "    verbose=True,\r\n",
        ")\r\n",
        "\r\n",
        "\r\n",
        "max_score = 0\r\n",
        "\r\n",
        "for i in range(0, 40):\r\n",
        "    \r\n",
        "    print('\\nEpoch: {}'.format(i))\r\n",
        "    train_logs = train_epoch.run(train_loader)\r\n",
        "    valid_logs = valid_epoch.run(valid_loader)\r\n",
        "    \r\n",
        "    # do something (save model, change lr, etc.)\r\n",
        "    if max_score < valid_logs['iou_score']:\r\n",
        "        max_score = valid_logs['iou_score']\r\n",
        "        torch.save(model, './best_model.pth')\r\n",
        "        print('Model saved!')\r\n",
        "        \r\n",
        "    if i == 25:\r\n",
        "        optimizer.param_groups[0]['lr'] = 1e-5\r\n",
        "        print('Decrease decoder learning rate to 1e-5!')\r\n",
        "\r\n",
        "    writer.add_scalar('Loss/train', train_logs['dice_loss'], i)\r\n",
        "    writer.add_scalar('Loss/valid', valid_logs['dice_loss'], i)\r\n",
        "    writer.add_scalar('Accuracy/train', train_logs['iou_score'], i)\r\n",
        "    writer.add_scalar('Accuracy/valid', valid_logs['iou_score'], i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MD6w5kVC7yBe"
      },
      "source": [
        "! kill -9 828"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXO_wJsqEv-j"
      },
      "source": [
        "del /q %TMP%.tensorboard-info*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x82sFxcLFvk_"
      },
      "source": [
        "%reload_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9SPOGKBFzSI"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcdIiAPCKfcs"
      },
      "source": [
        "import os\r\n",
        "import torch\r\n",
        "import segmentation_models_pytorch as smp\r\n",
        "import numpy as np\r\n",
        "import cv2\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import albumentations as albu\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torch.utils.data import Dataset as BaseDataset\r\n",
        "\r\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\r\n",
        "\r\n",
        "DATA_DIR = './gdrive/MyDrive/Colab Notebooks/SYNTHIA4/'\r\n",
        "\r\n",
        "x_test_dir = os.path.join(DATA_DIR, 'Test/RGB')\r\n",
        "y_test_dir = os.path.join(DATA_DIR, 'Test/SemSeg')\r\n",
        "\r\n",
        "best_model = torch.load('./best_model.pth')\r\n",
        "\r\n",
        "# create test dataset\r\n",
        "test_dataset = Dataset(\r\n",
        "    x_test_dir, \r\n",
        "    y_test_dir, \r\n",
        "    augmentation=get_validation_augmentation(), \r\n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\r\n",
        "    classes=CLASSES,\r\n",
        ")\r\n",
        "\r\n",
        "test_dataloader = DataLoader(test_dataset)\r\n",
        "\r\n",
        "# evaluate model on test set\r\n",
        "test_epoch = smp.utils.train.ValidEpoch(\r\n",
        "    model=best_model,\r\n",
        "    loss=loss,\r\n",
        "    metrics=metrics,\r\n",
        "    device=DEVICE,\r\n",
        ")\r\n",
        "\r\n",
        "logs = test_epoch.run(test_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5DGv_gjwWX6"
      },
      "source": [
        "test_dataset_vis = Dataset(\r\n",
        "    x_test_dir, y_test_dir, \r\n",
        "    classes=CLASSES,\r\n",
        ")\r\n",
        "\r\n",
        "color=[[255,0,0],[0,255,0],[0,0,255],[255,255,0],[255,0,255],\r\n",
        "        [0,255,255],[67,125,255],[125,200,67],[180,67,125],[67,40,0],[100,67,20],\r\n",
        "        [10,140,67],[67,210,19],[110,120,67]]\r\n",
        "\r\n",
        "for i in range(30):\r\n",
        "    \r\n",
        "    gt_img = np.zeros([480,640,3],dtype=np.uint8)\r\n",
        "    gt_img.fill(0) # or img[:] = 255\r\n",
        "\r\n",
        "    output_img = np.zeros([480,640,3],dtype=np.uint8)\r\n",
        "    output_img.fill(0) # or img[:] = 255\r\n",
        "\r\n",
        "    image_vis = test_dataset_vis[i][0].astype('uint8')\r\n",
        "    image, gt_mask = test_dataset[i]\r\n",
        "\r\n",
        "    gt_mask = gt_mask.squeeze()\r\n",
        "\r\n",
        "    for j in range(len(gt_mask)):\r\n",
        "        for x in range(480):\r\n",
        "            for y in range(640):\r\n",
        "                if gt_mask[j][x][y]==1:\r\n",
        "                    gt_img[x][y]=color[j]\r\n",
        "        \r\n",
        "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\r\n",
        "    pr_mask = best_model.predict(x_tensor)\r\n",
        "    pr_mask = (pr_mask.squeeze().cpu().numpy().round())\r\n",
        "    \r\n",
        "    for j in range(len(pr_mask)):\r\n",
        "        for x in range(480):\r\n",
        "            for y in range(640):\r\n",
        "                if pr_mask[j][x][y]==1:\r\n",
        "                    output_img[x][y]=color[j]\r\n",
        "    if i == 1:\r\n",
        "      visualize(\r\n",
        "        ground_truth_mask=gt_img, \r\n",
        "        predicted_mask=output_img\r\n",
        "        )\r\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33GOsBaBqYHb"
      },
      "source": [
        "visualize(\r\n",
        "    ground_truth_mask=gt_img, \r\n",
        "    predicted_mask=output_img\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVIxZjXa8Wto"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}